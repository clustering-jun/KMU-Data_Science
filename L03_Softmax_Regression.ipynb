{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN80oSeI7Sm4uwSztFSPJfO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clustering-jun/KMU-Data_Science/blob/main/L03_Softmax_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Softmax Regression Practice**\n"
      ],
      "metadata": {
        "id": "xXLCvW1WzmSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Softmax Regression Practice with Pytorch**"
      ],
      "metadata": {
        "id": "YGTHtkppzxRq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4BG2nhXzREo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x_train = torch.tensor([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5], [1,2,5,6], [1,6,6,6], [1,7,7,7] ], dtype=torch.float )\n",
        "y_train = torch.tensor([ [0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0], [1,0,0], [1,0,0] ], dtype=torch.float )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.randn(4,3, requires_grad=True) # 차원 4,3 --> raw: x_train의 차원과 동일하게, column: 맞출려고 하는 calss의 수\n",
        "b = torch.randn(1,3, requires_grad=True)\n",
        "\n",
        "optim = torch.optim.Adam([W,b], lr = 0.1)\n",
        "\n",
        "for epoch in range(3001):\n",
        "\n",
        "    h = torch.softmax(torch.mm(x_train, W) + b, dim = 1)\n",
        "    # h = (torch.mm(x_train, W) + b).softmax(dim=1)\n",
        "\n",
        "    cost = -torch.mean(torch.sum(y_train * torch.log(h), dim = 1))\n",
        "    # cost = -(y_train * torch.log(h)).sum(dim=1).mean()\n",
        "\n",
        "    optim.zero_grad()\n",
        "    cost.backward()\n",
        "    optim.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if epoch % 100 == 0:\n",
        "            print(f'epoch: {epoch}, cost: {cost.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a0tlJ4H9rs0",
        "outputId": "324f2f82-981a-4ff6-defa-0d56972d130a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 3.782303810119629\n",
            "epoch: 100, cost: 0.3121398389339447\n",
            "epoch: 200, cost: 0.21394389867782593\n",
            "epoch: 300, cost: 0.15235701203346252\n",
            "epoch: 400, cost: 0.11364435404539108\n",
            "epoch: 500, cost: 0.08802750706672668\n",
            "epoch: 600, cost: 0.07018798589706421\n",
            "epoch: 700, cost: 0.05724438652396202\n",
            "epoch: 800, cost: 0.0475451685488224\n",
            "epoch: 900, cost: 0.04008587449789047\n",
            "epoch: 1000, cost: 0.03422482684254646\n",
            "epoch: 1100, cost: 0.02953549101948738\n",
            "epoch: 1200, cost: 0.025724584236741066\n",
            "epoch: 1300, cost: 0.022585436701774597\n",
            "epoch: 1400, cost: 0.019968602806329727\n",
            "epoch: 1500, cost: 0.017764169722795486\n",
            "epoch: 1600, cost: 0.015889707952737808\n",
            "epoch: 1700, cost: 0.014282330870628357\n",
            "epoch: 1800, cost: 0.012893588282167912\n",
            "epoch: 1900, cost: 0.011685457080602646\n",
            "epoch: 2000, cost: 0.010628020390868187\n",
            "epoch: 2100, cost: 0.009697186760604382\n",
            "epoch: 2200, cost: 0.008873621001839638\n",
            "epoch: 2300, cost: 0.008141551166772842\n",
            "epoch: 2400, cost: 0.0074879443272948265\n",
            "epoch: 2500, cost: 0.006902116816490889\n",
            "epoch: 2600, cost: 0.006375034339725971\n",
            "epoch: 2700, cost: 0.0058993007987737656\n",
            "epoch: 2800, cost: 0.005468458868563175\n",
            "epoch: 2900, cost: 0.0050772069953382015\n",
            "epoch: 3000, cost: 0.0047209374606609344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측하기\n",
        "x_test = torch.tensor([[1,11,10,9], [1,3,4,3], [1,1,0,1]], dtype=torch.float)\n",
        "\n",
        "h_test = torch.softmax(torch.mm(x_test, W) + b, dim = 1)\n",
        "print(h_test)\n",
        "print(torch.argmax(h_test, dim = 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsCfX_svAeuZ",
        "outputId": "6b1a8e2b-125c-421b-fd4c-11938ae617d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000e+00, 6.6823e-17, 1.8649e-34],\n",
            "        [8.3073e-03, 7.4698e-01, 2.4471e-01],\n",
            "        [2.3333e-30, 6.5070e-11, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([0, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **조금 더 깔끔하게 Softmax**\n",
        "\n",
        "### **마음에 안드는 부분**\n",
        "- `[1,0,0], [0,1,0], [0,0,1]` 대신 `0, 1, 2`를 쓰면 안되나?\n",
        "- 이렇게 복잡한 함수(h, cost)를 항상 직접 구현해야되나? 어차피 softmax, cross entropy인데?"
      ],
      "metadata": {
        "id": "oBrSZJ6a68nS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "x_train = torch.tensor([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5], [1,2,5,6], [1,6,6,6], [1,7,7,7] ], dtype=torch.float )\n",
        "y_train = torch.tensor([2,2,2,1,1,1,0,0], dtype=torch.long)\n",
        "\n",
        "model = nn.Linear(4,3)\n",
        "optim = torch.optim.Adam(model.parameters(), lr = 0.1)\n",
        "\n",
        "\n",
        "for epoch in range(3001):\n",
        "\n",
        "    h = model(x_train)\n",
        "    cost = F.cross_entropy(h, y_train)\n",
        "\n",
        "    optim.zero_grad()\n",
        "    cost.backward()\n",
        "    optim.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if epoch % 100 == 0:\n",
        "            print(f'epoch: {epoch}, cost: {cost.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Usg0rGcfBpRm",
        "outputId": "7762448a-8252-4253-d584-2b05d6896ea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 2.5144050121307373\n",
            "epoch: 100, cost: 0.33555638790130615\n",
            "epoch: 200, cost: 0.2193930745124817\n",
            "epoch: 300, cost: 0.15191562473773956\n",
            "epoch: 400, cost: 0.11152353137731552\n",
            "epoch: 500, cost: 0.08549431711435318\n",
            "epoch: 600, cost: 0.06764886528253555\n",
            "epoch: 700, cost: 0.054840922355651855\n",
            "epoch: 800, cost: 0.04532477259635925\n",
            "epoch: 900, cost: 0.03805794566869736\n",
            "epoch: 1000, cost: 0.032382652163505554\n",
            "epoch: 1100, cost: 0.027865376323461533\n",
            "epoch: 1200, cost: 0.024210859090089798\n",
            "epoch: 1300, cost: 0.021212300285696983\n",
            "epoch: 1400, cost: 0.018721112981438637\n",
            "epoch: 1500, cost: 0.016628796234726906\n",
            "epoch: 1600, cost: 0.014854260720312595\n",
            "epoch: 1700, cost: 0.013336055912077427\n",
            "epoch: 1800, cost: 0.01202701497823\n",
            "epoch: 1900, cost: 0.010890303179621696\n",
            "epoch: 2000, cost: 0.00989691074937582\n",
            "epoch: 2100, cost: 0.009023749269545078\n",
            "epoch: 2200, cost: 0.008252129890024662\n",
            "epoch: 2300, cost: 0.0075670769438147545\n",
            "epoch: 2400, cost: 0.0069560641422867775\n",
            "epoch: 2500, cost: 0.006408879533410072\n",
            "epoch: 2600, cost: 0.0059170168824493885\n",
            "epoch: 2700, cost: 0.005473393481224775\n",
            "epoch: 2800, cost: 0.005071927793323994\n",
            "epoch: 2900, cost: 0.004707601387053728\n",
            "epoch: 3000, cost: 0.00437602074816823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **주의! F.cross_entropy는 softmax와 cross entropy를 합친 것.**"
      ],
      "metadata": {
        "id": "CKLGUo3dlSi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVHzxXADnOor",
        "outputId": "0fcd7a72-b7ce-4b9d-bc76-abdd665744c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=4, out_features=3, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h8Q14nznZ5x",
        "outputId": "51ca5c41-103e-4837-835b-37163376b818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x799f75b8b680>"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for a in model.parameters():\n",
        "    print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR-c0-dnnR2Y",
        "outputId": "4071eeea-b44f-46a7-f46d-60f7cbe5dd45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-12.2440,  -0.3130,   8.2319,  -1.9318],\n",
            "        [  7.2206,   0.3162,  -3.9238,   1.8617],\n",
            "        [  8.8315,   0.6994,  -9.8449,   0.5202]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-19.6614,   2.9228,  26.5391], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Softmax Regression with Scikit-learn**\n",
        "- sklearn에는 LogisticRegression에 Softmax regressin이 함께 구현됨.\n",
        "--> y에 두 종류 이상의 값이 있을 경우 softmax regression 실행"
      ],
      "metadata": {
        "id": "xS6kDmC1OJ-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "x_train = np.array([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5], [1,2,5,6], [1,6,6,6], [1,7,7,7] ])\n",
        "\n",
        "# y에 0, 1 ,2 등 둘 이상의 class가 존재 --> softmax regression\n",
        "y_train = np.array([2,2,2,1,1,1,0,0])\n",
        "\n",
        "model = LogisticRegression(penalty=None)\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# w와 b에 해당하는 값 출력\n",
        "print(model.coef_, model.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JbJRl2fOfzY",
        "outputId": "4c7bd9a5-f256-44ab-8f67-8758dbf88969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-59.67522386  -5.71584024  19.19765337   7.46552972]\n",
            " [ 10.79175325  -4.89176604  -0.41108063  11.03996337]\n",
            " [ 48.88347062  10.60760628 -18.78657274 -18.5054931 ]] [-27.82443948 -15.01027699  42.83471647]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측하기\n",
        "x_test = np.array([[1,11,10,9], [1,3,4,3], [1,1,0,1]])\n",
        "\n",
        "print(model.predict(x_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT53UVe-2T7v",
        "outputId": "d36ba7ae-dea4-4229-d56e-8b8c1ed23792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2]\n"
          ]
        }
      ]
    }
  ]
}